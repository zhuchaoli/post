---
title: 记一次线上网关问题与排查
categories: Java
date: 2021-08-10 22:29:09
tags: 网关故障排查
---

某天下午升级网关和后端服务后，出现大量请求无法通过网关，网关返回http 500异常状态，上线30分钟后进行回滚。<!-- more -->

## 原因分析

结合当天上线prometheus采集到的数据与测试环境测试结果发现，造成服务不可用的原因是网关频繁触发了熔断器，导致断路器一直处于打开状态，网关无法正常转发请求到后端服务。

造成熔断器长时间打开的原因有两个，一是高并发（并发1500左右），二是网关配置不合理，两个条件组合造成雪崩效应。

从prometheus采集到的网关数据来看，上线后30分钟内也不是所有请求都被拒绝了，还是有少量请求成功了，在熔断器关闭短暂的时间内，部分请求能正常被网关转发。

以下是上次上线时prometheus采集的部分数据指标，由图3能够看出，当时并发量超过1400（7000/5s)。 

![1.png](https://p.pstatp.com/origin/pgc-image/c99b83dcfd30497b88cf26fc7c5037f3)

图1 上线后网关发生熔断的次数 

![2.png](https://p.pstatp.com/origin/pgc-image/814a99c07098492d9c46e4113a551179)

图2 由熔断导致异常抛出的次数 

![3.png](https://p.pstatp.com/origin/pgc-image/0cd4150ef0d04dd492e905c9a9fa0ca5)

图3 断路器状态（一直处于打开状态，服务没有恢复） 

![4.png](https://p.pstatp.com/origin/pgc-image/504a54dfc9474b1080322220ee5b310a)

图4 被网关信号量拒绝的请求

 ![5.png](https://p.pstatp.com/origin/pgc-image/460da4022bfc4e4d9bbda5fbfaeef740)

图5 高并发情况下，熔断器无法关闭（测试环境）

## 测试记录

#### 测试环境信息

k8s环境，3台cce，每台4核8G内存

api-gateway  3个实例

cleanup-controller 2个实例

api-gateway和cleanup-controller使用的镜像都是上次上线出问题的镜像，没有修改代码。

#### 测试结果

测试工具：jmeter

修改配置前：

| 并发请求数 | 结果                             | 备注 |
| ---------- | -------------------------------- | ---- |
| 500*1      | 部分网关结点熔断，且恢复时间长   |      |
| 1000*1     | 所有服务发生熔断，长时间没有恢复 |      |

修改配置后：

| 并发请求数 | 结果                           | 备注                       |
| ---------- | ------------------------------ | -------------------------- |
| 1500*2     | 没有发生熔断情况，所有请求正常 | 部分请求返回较慢，排队处理 |
| 3000*2     | 没有发生熔断情况，所有请求正常 |                            |

## 解决方案

将网关以下的两个配置独立出来，与原来的springboot.cloud配置分开，提高网关抗并发能力和容错能力。

```properties
# zuul 单点处理信号量默认100，提高至2000
zuul.semaphore.max-semaphores=2000
# 断路器恢复窗口设置为5秒，原来是3ms
hystrix.command.default.circuitBreaker.sleepWindowInMilliseconds=5000
```

第一个配置max-semaphores是hystrix最大处理信号量（每秒），如果超过了这个数，hystrix会直接拒绝请求，并记入错误次数中，如果5秒内错误超过50，就会发生熔断。这个配置默认是100。另外由于线上配置tomcat线程数是600（server.tomcat.max-threads=600），而信号量使用的是100，远小于最大线程数，这也是容易造成熔断的原因，信号量需要大于最大线程数。

第二个配置sleepWindowInMilliseconds是发生熔断后的睡眠窗口，过了这个时间窗口，请求才能再次被转发给后端服务。

## 疑问

为什么zuul+eureka没有问题，zuul+k8s会有问题？

测试环境zuul+eureka并发1000请求没有压力，而zuul+k8s就会发生异常情况，具体细节未知。因为这次不止是把eureka换成了k8s服务发现，还升级了spring cloud，zuul、hystrix、ribbon这些组件都跟着升级了，猜测应该是spring cloud升级后，熔断机制与旧版有所区别，特别是spring cloud的配置。

## 进一步分析

虽然增大信号量配置能提高网关并发承受能力，但经测试发现，如果并发超过了信号量范围，并持续一段时间，网关会出现挂掉情况（假死，无任何响应）。

按网关路由转发流程逐个分析zuul、hystrix、ribbon、httpclient源码，并本地debug，最后定位问题出在http线程池上，高并发情况下，很多请求直接抛出异常，导致线程池的连接无法回收，后面的请求无线程可用，进入无限等待状态。

https://github.com/spring-cloud/spring-cloud-netflix/issues/2814

解决方案是将ribbon默认使用的httpclient换成okhttp。

```properties
# 禁用httpclient
ribbon.httpclient.enabled=false
# 启用okhttp
ribbon.okhttp.enabled=true
```


